{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction to XGBoost for Classification ðŸš€\n",
    "\n",
    "**XGBoost (eXtreme Gradient Boosting)** is a highly efficient and powerful machine learning algorithm based on the **Gradient Boosting** framework. It has become a go-to choice for many data scientists and is a frequent winner of machine learning competitions.\n",
    "\n",
    "### Why is XGBoost so popular?\n",
    "* **Performance:** It is known for its state-of-the-art performance on a wide range of structured data problems.\n",
    "* **Speed:** It is optimized for computational efficiency and can be much faster than standard gradient boosting implementations.\n",
    "* **Flexibility:** It includes built-in regularization to prevent overfitting, can handle missing values, and offers many hyperparameters for fine-tuning.\n",
    "\n",
    "Like other boosting methods, XGBoost works by building a series of decision trees **sequentially**. Each new tree is trained to correct the errors made by the previous trees, allowing the model to learn complex patterns and make highly accurate predictions.\n",
    "\n",
    "This notebook will compare the performance of a simple Logistic Regression model against an XGBoost classifier on a synthetic multi-class dataset.\n"
   ],
   "id": "d5ebda1a99f95080"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Generating a Multi-Class Dataset\n",
    "\n",
    "First, we'll use `scikit-learn` to generate a synthetic dataset for a three-class classification problem. The dataset will have 10,000 samples and 10 features.\n"
   ],
   "id": "96a12c66a0b5722b"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-30T17:23:26.986712Z",
     "start_time": "2025-09-30T17:23:26.916820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=10,\n",
    "    n_informative=8,\n",
    "    n_redundant=2,\n",
    "    n_repeated=0,\n",
    "    n_classes=3,\n",
    "    random_state=42\n",
    ")"
   ],
   "id": "678457f176129e98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.70      0.72       677\n",
      "           1       0.76      0.77      0.76       664\n",
      "           2       0.68      0.71      0.70       659\n",
      "\n",
      "    accuracy                           0.73      2000\n",
      "   macro avg       0.73      0.73      0.73      2000\n",
      "weighted avg       0.73      0.73      0.73      2000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's inspect the shape and a few samples of our generated data.",
   "id": "1e6cd7f8b131d85d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:20:35.887609Z",
     "start_time": "2025-09-30T17:20:35.882804Z"
    }
   },
   "cell_type": "code",
   "source": "X.shape",
   "id": "b66fdeea53890157",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:22:36.980549Z",
     "start_time": "2025-09-30T17:22:36.976243Z"
    }
   },
   "cell_type": "code",
   "source": "X[:2]",
   "id": "4ff25c5db8c6ba8c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.31573515,  0.6775586 , -4.43495008, -1.755074  , -0.47264511,\n",
       "        -2.96504643,  2.39563871, -0.38616042, -5.99696616,  2.70706827],\n",
       "       [-1.71149777,  1.42608068, -0.56808572,  1.19785018, -1.45465463,\n",
       "         2.03940975, -1.64207421,  0.54053374, -1.52128605,  1.09364584]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:22:45.670087Z",
     "start_time": "2025-09-30T17:22:45.666087Z"
    }
   },
   "cell_type": "code",
   "source": "y[:2]",
   "id": "6d2a2a1e28272a01",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Baseline Model: Logistic Regression\n",
    "\n",
    "Before using XGBoost, let's establish a performance baseline with a simpler model, `LogisticRegression`. We'll split our data and train the model.\n"
   ],
   "id": "c8cf0a8df78d924f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:23:35.648418Z",
     "start_time": "2025-09-30T17:23:35.621197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"--- Logistic Regression Report ---\")\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "id": "229f7cc924909f64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.70      0.72       677\n",
      "           1       0.76      0.77      0.76       664\n",
      "           2       0.68      0.71      0.70       659\n",
      "\n",
      "    accuracy                           0.73      2000\n",
      "   macro avg       0.73      0.73      0.73      2000\n",
      "weighted avg       0.73      0.73      0.73      2000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T13:56:06.327272Z",
     "start_time": "2025-09-01T13:56:06.323145Z"
    }
   },
   "cell_type": "markdown",
   "source": "The baseline Logistic Regression model achieves an accuracy of **73%**.",
   "id": "f503c1dfeefb48fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Training the XGBoost Classifier\n",
    "\n",
    "Now, let's train an `XGBClassifier` on the same data. The XGBoost library provides an easy-to-use, `scikit-learn`-compatible interface.\n"
   ],
   "id": "6c90adbb16cfe908"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T17:24:15.256621Z",
     "start_time": "2025-09-30T17:24:14.558033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"--- XGBoost Classifier Report ---\")\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "id": "9cca51513e4eda8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- XGBoost Classifier Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89       677\n",
      "           1       0.91      0.91      0.91       664\n",
      "           2       0.91      0.90      0.91       659\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.90      0.90      0.90      2000\n",
      "weighted avg       0.90      0.90      0.90      2000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T13:56:07.607275Z",
     "start_time": "2025-09-01T13:56:07.603868Z"
    }
   },
   "cell_type": "markdown",
   "source": "The XGBoost model achieves an accuracy of **90%**, a dramatic improvement over the baseline.\n",
   "id": "c914d09d1fe2a060"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T13:56:11.037167Z",
     "start_time": "2025-09-01T13:56:11.010386Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "| Model | Accuracy |\n",
    "|:--- |:--- |\n",
    "| Logistic Regression | 73% |\n",
    "| **XGBoost Classifier** | **90%** |\n",
    "\n",
    "This comparison clearly demonstrates the power of XGBoost. For complex classification tasks, advanced ensemble methods like XGBoost can provide a substantial performance boost over simpler linear models. While XGBoost is more complex under the hood, its default settings often provide excellent results, making it a powerful and accessible tool."
   ],
   "id": "f9d88fcadefaacb3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
