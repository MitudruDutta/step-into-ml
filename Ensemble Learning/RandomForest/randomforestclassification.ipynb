{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Improving Performance with Ensemble Learning: Voting and Random Forests ðŸŒ³\n",
    "\n",
    "**Ensemble Learning** is a powerful machine learning concept based on a simple idea: by combining the predictions of multiple individual models (called \"base estimators\"), we can often create a final model that is more accurate and robust than any of the individual models on their own.\n",
    "\n",
    "The intuition behind this is the \"wisdom of the crowd.\" The collective opinion of a diverse group is often better than the opinion of a single expert. Similarly, ensemble methods combine the \"votes\" of several models to average out their individual weaknesses and produce a better overall prediction.\n",
    "\n",
    "This notebook will demonstrate two popular ensemble techniques:\n",
    "1.  **Voting Classifier:** Combines predictions from different types of models.\n",
    "2.  **Random Forest:** An ensemble of many Decision Tree models.\n",
    "\n",
    "We will use the Raisin dataset to see how these ensemble methods improve upon the performance of individual baseline models.\n"
   ],
   "id": "ec726b1634bcf23f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. The Dataset and Baseline Performance\n",
    "\n",
    "First, we'll load the Raisin dataset and train two individual models, a Support Vector Machine (SVM) and a Decision Tree, to establish a baseline performance that we can try to beat with our ensembles.\n"
   ],
   "id": "f818831b7a88104e"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-29T14:33:47.104212Z",
     "start_time": "2025-09-29T14:33:46.424212Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    Area  MajorAxisLength  MinorAxisLength  Eccentricity  ConvexArea  \\\n",
       "0  87524       442.246011       253.291155      0.819738       90546   \n",
       "1  75166       406.690687       243.032436      0.801805       78789   \n",
       "2  90856       442.267048       266.328318      0.798354       93717   \n",
       "3  45928       286.540559       208.760042      0.684989       47336   \n",
       "4  79408       352.190770       290.827533      0.564011       81463   \n",
       "\n",
       "     Extent  Perimeter    Class  \n",
       "0  0.758651   1184.040  Kecimen  \n",
       "1  0.684130   1121.786  Kecimen  \n",
       "2  0.637613   1208.575  Kecimen  \n",
       "3  0.699599    844.162  Kecimen  \n",
       "4  0.792772   1073.251  Kecimen  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87524</td>\n",
       "      <td>442.246011</td>\n",
       "      <td>253.291155</td>\n",
       "      <td>0.819738</td>\n",
       "      <td>90546</td>\n",
       "      <td>0.758651</td>\n",
       "      <td>1184.040</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75166</td>\n",
       "      <td>406.690687</td>\n",
       "      <td>243.032436</td>\n",
       "      <td>0.801805</td>\n",
       "      <td>78789</td>\n",
       "      <td>0.684130</td>\n",
       "      <td>1121.786</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90856</td>\n",
       "      <td>442.267048</td>\n",
       "      <td>266.328318</td>\n",
       "      <td>0.798354</td>\n",
       "      <td>93717</td>\n",
       "      <td>0.637613</td>\n",
       "      <td>1208.575</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45928</td>\n",
       "      <td>286.540559</td>\n",
       "      <td>208.760042</td>\n",
       "      <td>0.684989</td>\n",
       "      <td>47336</td>\n",
       "      <td>0.699599</td>\n",
       "      <td>844.162</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79408</td>\n",
       "      <td>352.190770</td>\n",
       "      <td>290.827533</td>\n",
       "      <td>0.564011</td>\n",
       "      <td>81463</td>\n",
       "      <td>0.792772</td>\n",
       "      <td>1073.251</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('Raisin_Dataset.xlsx')\n",
    "df.head()"
   ],
   "id": "c1635bd053404efb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T14:33:52.614894Z",
     "start_time": "2025-09-29T14:33:52.563388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = df[['Area', 'MajorAxisLength', 'MinorAxisLength', 'Eccentricity', 'ConvexArea', 'Extent', 'Perimeter']]\n",
    "y = df['Class']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ],
   "id": "4174924690fe5cd1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Baseline Model 1: Support Vector Machine (SVM)",
   "id": "1428c5ac519cf735"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T14:37:39.941602Z",
     "start_time": "2025-09-29T14:37:39.924554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model_svm = SVC(kernel='rbf')\n",
    "model_svm.fit(X_train, y_train)\n",
    "y_pred_svm = model_svm.predict(X_test)\n",
    "print(\"--- SVM Baseline Report ---\")\n",
    "print(classification_report(y_test, y_pred_svm))"
   ],
   "id": "4c52342ed8216c1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SVM Baseline Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Besni       0.86      0.75      0.80        83\n",
      "     Kecimen       0.81      0.90      0.85        97\n",
      "\n",
      "    accuracy                           0.83       180\n",
      "   macro avg       0.83      0.82      0.82       180\n",
      "weighted avg       0.83      0.83      0.83       180\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Baseline Model 2: Decision Tree",
   "id": "156768986a483bdf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T14:38:03.479840Z",
     "start_time": "2025-09-29T14:38:03.466806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model_dt = DecisionTreeClassifier()\n",
    "model_dt.fit(X_train, y_train)\n",
    "y_pred_dt = model_dt.predict(X_test)\n",
    "print(\"--- Decision Tree Baseline Report ---\")\n",
    "print(classification_report(y_test, y_pred_dt))"
   ],
   "id": "9990012bc8ce45b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Decision Tree Baseline Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Besni       0.83      0.77      0.80        83\n",
      "     Kecimen       0.82      0.87      0.84        97\n",
      "\n",
      "    accuracy                           0.82       180\n",
      "   macro avg       0.82      0.82      0.82       180\n",
      "weighted avg       0.82      0.82      0.82       180\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Both baseline models achieve an accuracy of around **83%**. Let's see if we can improve on this with ensembles.\n",
   "id": "495c74976b3bd155"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Combining Diverse Models: The Voting Classifier\n",
    "\n",
    "A Voting Classifier combines different types of models (e.g., Logistic Regression, SVM, and a Decision Tree) and makes a prediction based on their combined output.\n",
    "\n",
    "### a) Hard Voting\n",
    "Hard voting is a simple majority rule. The final prediction is the class label that was predicted most frequently by the individual models.\n"
   ],
   "id": "f55707724fda5b4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T14:38:37.516116Z",
     "start_time": "2025-09-29T14:38:37.431976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "svc_model = SVC(kernel='rbf', probability=True) # probability=True is needed for soft voting later\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# Hard Voting (default)\n",
    "vc_hard = VotingClassifier(estimators=[('lr', log_model), ('svc', svc_model), ('dt', dt_model)])\n",
    "vc_hard.fit(X_train, y_train)\n",
    "y_pred_hard = vc_hard.predict(X_test)\n",
    "\n",
    "print(\"--- Hard Voting Report ---\")\n",
    "print(classification_report(y_test, y_pred_hard))"
   ],
   "id": "9877299be97cad27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Hard Voting Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Besni       0.92      0.83      0.87        83\n",
      "     Kecimen       0.87      0.94      0.90        97\n",
      "\n",
      "    accuracy                           0.89       180\n",
      "   macro avg       0.89      0.88      0.89       180\n",
      "weighted avg       0.89      0.89      0.89       180\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "By combining the three models, our accuracy improves significantly from 83% to **89%**.\n",
   "id": "70c7aa6aca43e7d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### b) Soft Voting\n",
    "\n",
    "Soft voting is often more powerful. It averages the predicted probabilities from each model and then chooses the class with the highest average probability. This method gives more weight to highly confident votes.\n"
   ],
   "id": "fba5b45ffd515b17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T14:39:13.295296Z",
     "start_time": "2025-09-29T14:39:13.211705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Soft Voting\n",
    "vc_soft = VotingClassifier(estimators=[('lr', log_model), ('svc', svc_model), ('dt', dt_model)], voting='soft')\n",
    "vc_soft.fit(X_train, y_train)\n",
    "y_pred_soft = vc_soft.predict(X_test)\n",
    "\n",
    "print(\"--- Soft Voting Report ---\")\n",
    "print(classification_report(y_test, y_pred_soft))"
   ],
   "id": "790cc9aa405985cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Soft Voting Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Besni       0.92      0.82      0.87        83\n",
      "     Kecimen       0.86      0.94      0.90        97\n",
      "\n",
      "    accuracy                           0.88       180\n",
      "   macro avg       0.89      0.88      0.88       180\n",
      "weighted avg       0.89      0.88      0.88       180\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this case, soft voting achieved an accuracy of **87%**, which is still a strong improvement over the baseline models.\n",
   "id": "520a10fb740a8cf1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. An Ensemble of Similar Models: The Random Forest\n",
    "\n",
    "A **Random Forest** is an ensemble that consists of a large number of individual **Decision Trees**. It uses two key techniques to create a diverse set of trees:\n",
    "1.  **Bagging (Bootstrap Aggregating):** Each tree is trained on a random subsample of the training data.\n",
    "2.  **Feature Randomness:** At each node, only a random subset of features is considered for making the split.\n",
    "\n",
    "These two sources of randomness produce a \"forest\" of decorrelated trees. Their collective prediction is more accurate and much less prone to overfitting than a single decision tree.\n"
   ],
   "id": "c9ac99d47a4599b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T14:39:43.139214Z",
     "start_time": "2025-09-29T14:39:43.096112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# n_estimators is the number of trees in the forest\n",
    "model_rf = RandomForestClassifier(n_estimators=20)\n",
    "model_rf.fit(X_train, y_train)\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "print(\"--- Random Forest Report ---\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ],
   "id": "2c62107a2c6c64cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Random Forest Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Besni       0.83      0.84      0.84        83\n",
      "     Kecimen       0.86      0.86      0.86        97\n",
      "\n",
      "    accuracy                           0.85       180\n",
      "   macro avg       0.85      0.85      0.85       180\n",
      "weighted avg       0.85      0.85      0.85       180\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The Random Forest achieves an accuracy of **86%**, a notable improvement over the single Decision Tree's 83%.\n",
   "id": "9b71ec81e156f478"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "| Model | Accuracy |\n",
    "|:--- |:--- |\n",
    "| Single SVM | 83% |\n",
    "| Single Decision Tree | 83% |\n",
    "| **Random Forest Ensemble** | **86%** |\n",
    "| **Voting Classifier (Soft)** | **87%** |\n",
    "| **Voting Classifier (Hard)** | **89%** |\n",
    "\n",
    "This experiment clearly shows the power of ensemble learning. By combining multiple models, we were able to create a classifier that outperformed any single model on its own."
   ],
   "id": "f8bdd2aae231efe9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
