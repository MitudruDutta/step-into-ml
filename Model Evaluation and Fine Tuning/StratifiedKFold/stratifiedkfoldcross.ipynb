{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Stratified K-Fold for Imbalanced Classification ⚖️\n",
    "\n",
    "**K-Fold Cross-Validation** is a robust method for evaluating machine learning models. However, the standard K-Fold technique can be problematic when dealing with **imbalanced datasets**, where one class is much more frequent than the others.\n",
    "\n",
    "### The Problem with Imbalanced Data\n",
    "\n",
    "Standard K-Fold splits the data randomly without considering the class distribution. In an imbalanced dataset, this can lead to some \"folds\" (test sets) having a very different proportion of classes than the original dataset. In extreme cases, a fold might not contain any samples from the minority class at all! This can result in misleading and unstable performance scores.\n",
    "\n",
    "### The Solution: Stratified K-Fold\n",
    "\n",
    "**Stratified K-Fold Cross-Validation** is a variation of K-Fold designed specifically for this problem. It ensures that each fold has the **same percentage of samples for each class** as is present in the original dataset. This guarantees that every test set is a representative sample of the overall class distribution, leading to more reliable and trustworthy model evaluation.\n",
    "\n",
    "---"
   ],
   "id": "b6275de86657f94"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Creating an Imbalanced Dataset\n",
    "\n",
    "First, let's create a synthetic dataset where the minority class (class 1) makes up only 10% of the data. We use `make_classification` with the `weights` parameter to achieve this.\n"
   ],
   "id": "7d4176fa1dc76de6"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-05T20:42:44.645816Z",
     "start_time": "2025-10-05T20:42:44.643048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "3a210921a5bcbcae",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T20:42:48.735156Z",
     "start_time": "2025-10-05T20:42:48.729625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = make_classification(\n",
    "    n_features=10,\n",
    "    n_samples=1000,\n",
    "    n_informative=8,\n",
    "    n_redundant=2,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    weights=[0.9, 0.1],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ],
   "id": "2a2056d05858392c",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can use `Counter` to confirm the imbalance.",
   "id": "147e4e7253de07fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T20:43:10.454346Z",
     "start_time": "2025-10-05T20:43:10.450081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(y)"
   ],
   "id": "210c24195d44c437",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 897, 1: 103})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. The Issue with Standard K-Fold\n",
    "\n",
    "Now, let's see what happens when we apply a standard `KFold` cross-validator to our imbalanced data. We will check the class distribution in each of the 5 test folds.\n"
   ],
   "id": "464800e815d0ab6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T20:43:26.875565Z",
     "start_time": "2025-10-05T20:43:26.871122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    y_test = y[test_index]\n",
    "    print(Counter(y_test))"
   ],
   "id": "cb2d080b94bc1f00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 177, 1: 23})\n",
      "Counter({0: 179, 1: 21})\n",
      "Counter({0: 183, 1: 17})\n",
      "Counter({0: 181, 1: 19})\n",
      "Counter({0: 177, 1: 23})\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Observation:** Notice the significant variation in the count of the minority class (1) across the folds, ranging from just 17 to 23. Evaluating a model on these inconsistent test sets can lead to unreliable scores.\n",
   "id": "3de96f645e486072"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. The Solution: Stratified K-Fold\n",
    "\n",
    "`StratifiedKFold` ensures that the class ratio is preserved in each fold.\n"
   ],
   "id": "8dfd59c90bf4e1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T20:44:05.335217Z",
     "start_time": "2025-10-05T20:44:05.330432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    y_test = y[test_index]\n",
    "    print(Counter(y_test))"
   ],
   "id": "b886fb7685446ce7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 180, 1: 20})\n",
      "Counter({0: 180, 1: 20})\n",
      "Counter({0: 179, 1: 21})\n",
      "Counter({0: 179, 1: 21})\n",
      "Counter({0: 179, 1: 21})\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Observation:** With stratification, the number of minority class samples in each fold is much more consistent (either 20 or 21), providing a stable basis for model evaluation.\n",
   "id": "6e01ead730534bbc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Evaluating Models with Stratified Folds\n",
    "\n",
    "We can now use our `StratifiedKFold` object (`skf`) with `cross_val_score` to get a reliable performance estimate for different models.\n"
   ],
   "id": "93e058aac5c14fcd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### a) Logistic Regression",
   "id": "6fa2d3c6532b36d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T20:44:46.785496Z",
     "start_time": "2025-10-05T20:44:46.768472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores_logistic = cross_val_score(LogisticRegression(), X, y, cv=skf, scoring='accuracy')\n",
    "np.average(scores_logistic)"
   ],
   "id": "5682e4d663f711b9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9019999999999999"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### b) Decision Tree Classifier",
   "id": "9515300f344eb9fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T20:44:59.432236Z",
     "start_time": "2025-10-05T20:44:59.395427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "scores_dt = cross_val_score(DecisionTreeClassifier(), X, y, cv=skf, scoring='accuracy')\n",
    "np.average(scores_dt)"
   ],
   "id": "32f0922e654315f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8940000000000001"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### c) Random Forest Classifier",
   "id": "6f33698daa5b517b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T20:45:25.550957Z",
     "start_time": "2025-10-05T20:45:25.454133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "scores_rf = cross_val_score(RandomForestClassifier(n_estimators=10), X, y, cv=skf, scoring='accuracy')\n",
    "np.average(scores_rf)"
   ],
   "id": "1d7f510fa14af595",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9129999999999999"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### A Quick Shortcut\n",
    "\n",
    "For classification tasks, `scikit-learn` is smart! If you simply pass an integer to the `cv` parameter in `cross_val_score`, it will automatically use `StratifiedKFold` by default.\n"
   ],
   "id": "32cc09781dba835b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T20:45:51.610210Z",
     "start_time": "2025-10-05T20:45:51.513445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Passing cv=5 automatically uses StratifiedKFold for classifiers\n",
    "cross_val_score(RandomForestClassifier(n_estimators=10), X, y, cv=5, scoring=\"accuracy\")"
   ],
   "id": "f06128b7a84038c0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92 , 0.895, 0.905, 0.92 , 0.915])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "When working with **imbalanced classification datasets**, you should always use **Stratified K-Fold Cross-Validation** instead of standard K-Fold. It ensures that each evaluation fold is a representative sample of the overall class distribution, leading to more reliable and trustworthy model performance estimates."
   ],
   "id": "54a7b00bd8ba0560"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
