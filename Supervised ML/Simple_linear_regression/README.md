# ğŸ“– Simple Linear Regression: Predicting Home Prices

## Introduction

Simple Linear Regression is a fundamental algorithm in machine learning used to model the relationship between a single independent variable (feature) and a dependent variable (target) by fitting a linear equation to the observed data.

This project serves as a beginner-friendly introduction to implementing Simple Linear Regression to predict home prices based on their square footage.

## ğŸ§  Theory

### Key Concepts

The goal of simple linear regression is to find the best-fitting straight line through the data points. This line is represented by the equation:

**`y = mx + c`**

- **y**: The dependent variable (what we want to predict, i.e., `price`).
- **x**: The independent variable (the feature we use for prediction, i.e., `area`).
- **m**: The slope or **coefficient** of the line. It represents the change in `y` for a one-unit change in `x`.
- **c**: The **intercept**, which is the value of `y` when `x` is 0.

### Ordinary Least Squares (OLS)

The algorithm finds the optimal values for `m` and `c` by minimizing a cost function. The most common method is **Ordinary Least Squares (OLS)**, which minimizes the **Sum of Squared Errors (SSE)** (also called Residual Sum of Squares).

The error (or residual) for each data point is the difference between the actual value (`y_i`) and the predicted value (`Å·_i`).

**Error (Residual):** `e_i = y_i - Å·_i`

**Cost Function (SSE):**
`SSE = Î£(y_i - Å·_i)Â² = Î£(y_i - (mx_i + c))Â²`

OLS finds the `m` and `c` that make this sum as small as possible. The closed-form solution is:

**Slope (m):**
`m = Î£((x_i - xÌ„)(y_i - È³)) / Î£(x_i - xÌ„)Â²`

**Intercept (c):**
`c = È³ - m * xÌ„`

Where:
- `xÌ„` is the mean of the independent variable `x`.
- `È³` is the mean of the dependent variable `y`.

### ğŸ“ Evaluation Metrics

Once the model is trained, we evaluate how well it generalizes:

```
Residual (e_i) = y_i - Å·_i
MSE = (1/n) Î£ (y_i - Å·_i)Â²
RMSE = âˆšMSE
MAE = (1/n) Î£ |y_i - Å·_i|
RÂ² = 1 âˆ’ Î£(y_i âˆ’ Å·_i)Â² / Î£(y_i âˆ’ È³)Â²
```

- **MSE / RMSE**: Penalize larger errors more (squared term). RMSE in original units.
- **MAE**: Robust to outliers compared to MSE.
- **RÂ²**: Proportion of variance explained (can be negative if model performs worse than predicting the mean).

### âœ… Core Assumptions (for inference / reliability)

1. **Linearity**: Relationship between X and y is linear.
2. **Independence**: Residuals are independent (no autocorrelation).
3. **Homoscedasticity**: Constant variance of residuals across X.
4. **Normality of residuals** (mainly for confidence intervals / hypothesis tests).
5. **No influential outliers** distorting the fit.

(For pure prediction with ML mindset, mild violations may be acceptableâ€”but diagnostics still help.)

### ğŸ” Diagnostic Checks

- Scatter plot of `area` vs `price` â†’ assesses linearity.
- Residuals vs predicted values â†’ look for random scatter (no patterns).
- Histogram / KDE or QQ plot of residuals â†’ normality check (optional).
- Leverage/outlier detection: Cookâ€™s distance (advanced extension).

## ğŸ§ª Minimal scikit-learn Example

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load data
df = pd.read_csv('home_prices.csv')
X = df[['area_sqr_ft']]
y = df['price_lakhs']

# Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train
model = LinearRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Metrics
mse = mean_squared_error(y_test, y_pred)
rmse = mse ** 0.5
r2 = r2_score(y_test, y_pred)
print(f"RMSE: {rmse:.3f} | RÂ²: {r2:.3f}")
```

## ğŸ“Š Dataset

- **File**: `home_prices.csv`
- **Description**: A simple dataset containing two columns:
  - `area_sqr_ft`: The area of the house in square feet (Independent Variable).
  - `price_lakhs`: The price of the house in Lakhs (Dependent Variable).

## ğŸ›  Implementation Steps

1. **Load Data**: The `home_prices.csv` dataset is loaded using pandas.
2. **Data Exploration**: Explore basic stats and inspect for anomalies/outliers.
3. **Visualization**: Scatter plot of `area_sqr_ft` vs `price_lakhs` to confirm linear trend.
4. **Train-Test Split**: Split to evaluate on unseen data (prevents optimistic bias).
5. **Model Training**: Fit `LinearRegression` from scikit-learn.
6. **Prediction**: Generate predictions on the test set.
7. **Evaluation**: Compute RMSE, RÂ²; plot regression line and residuals.
8. (Optional) **Diagnostics**: Residual plot, leverage points, transform variables if non-linear.

## âš ï¸ Common Pitfalls

- Extrapolating beyond the observed range of `area_sqr_ft`.
- Ignoring outliers that skew slope.
- RÂ² obsession: High RÂ² doesnâ€™t imply causal relationship.
- Using a single train/test split instead of cross-validation for small datasets.
- Not checking residual patterns (hidden non-linearity).

## ğŸš€ Running the Notebook

From the project root (after installing dependencies):

```batch
jupyter notebook "Supervised ML/Simple_linear_regression/linear_regression_single_variable.ipynb"
```

Or open via the Jupyter UI.

## ğŸ”„ Extensions

- Add **multiple features** (bedrooms, location) â†’ Multiple Linear Regression.
- Try **polynomial features** if curvature appears.
- Apply **regularization** (Ridge/Lasso) if expanding features.
- Use **log transformation** if variance increases with area.
- Add **confidence intervals** for predictions (via statsmodels or bootstrapping).

## âœ… Key Takeaways

- **Strengths**: Simple to implement, easily interpretable, and provides a good baseline.
- **Limitations**: Assumes linearity; sensitive to outliers; weak under complex relationships.
- **Best Practice**: Always visualize and validate assumptions before trusting inferences.

## ğŸ“‚ Files

- `linear_regression_single_variable.ipynb`: The Jupyter Notebook with the Python code and explanations.
- `home_prices.csv`: The dataset used for training and testing.

---

## ğŸ§¾ Summary

This module introduces the simplest predictive model: fit a line minimizing squared errors, evaluate with RMSE and RÂ², verify assumptions, and build intuition before moving to richer models.
