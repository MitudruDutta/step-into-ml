{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Streamlining Workflows with Scikit-Learn Pipelines ðŸš€\n",
    "\n",
    "Most machine learning projects involve a sequence of steps: loading data, splitting it, preprocessing (like feature scaling), and finally, training a model. Managing these steps manually can be tedious and, more importantly, can lead to common mistakes like **data leakage**.\n",
    "\n",
    "### The Problem with Manual Steps\n",
    "\n",
    "A frequent error is to apply a preprocessing step, like scaling, to the entire dataset *before* splitting it into training and testing sets. This causes the scaler to learn from the test data, \"leaking\" information into the training process and leading to an overly optimistic evaluation of the model's performance. The correct method is to fit the scaler *only* on the training data and then use it to transform both the training and test sets.\n",
    "\n",
    "### The Solution: `Pipeline`\n",
    "\n",
    "A **`scikit-learn` Pipeline** solves this problem by chaining multiple steps together into a single \"meta-estimator\". It bundles preprocessing and modeling into one object, ensuring that the steps are always performed in the correct order and that data leakage is prevented.\n",
    "\n",
    "This notebook will first demonstrate the manual process of scaling and training an SVM and then show how to achieve the same result more efficiently and safely with a `Pipeline`.\n",
    "\n",
    "---"
   ],
   "id": "7503bcb0ba548230"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. The Manual Workflow: Scaling and Training Separately\n",
    "\n",
    "We will use the Raisin dataset. The features have different scales, so scaling is an important step before using a distance-based algorithm like SVM.\n",
    "\n",
    "### Step 1.1: Data Loading and Splitting"
   ],
   "id": "781817efa004a06c"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-28T14:53:47.372515Z",
     "start_time": "2025-09-28T14:53:46.689687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from graphviz import pipe_lines\n",
    "\n",
    "df = pd.read_excel('Raisin_Dataset.xlsx')\n",
    "df.sample(5)"
   ],
   "id": "80239827d78a51ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Area  MajorAxisLength  MinorAxisLength  Eccentricity  ConvexArea  \\\n",
       "51   114648       508.128933       288.953981      0.822571      118314   \n",
       "292   72219       376.650492       249.529454      0.749065       74373   \n",
       "54   111450       478.310971       298.630592      0.781150      113256   \n",
       "765  121080       573.403612       270.632507      0.881612      124432   \n",
       "192   37569       232.427848       208.152006      0.444950       38874   \n",
       "\n",
       "       Extent  Perimeter    Class  \n",
       "51   0.681905   1340.897  Kecimen  \n",
       "292  0.777795   1050.221  Kecimen  \n",
       "54   0.690093   1298.188  Kecimen  \n",
       "765  0.723790   1418.385    Besni  \n",
       "192  0.794371    734.102  Kecimen  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>114648</td>\n",
       "      <td>508.128933</td>\n",
       "      <td>288.953981</td>\n",
       "      <td>0.822571</td>\n",
       "      <td>118314</td>\n",
       "      <td>0.681905</td>\n",
       "      <td>1340.897</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>72219</td>\n",
       "      <td>376.650492</td>\n",
       "      <td>249.529454</td>\n",
       "      <td>0.749065</td>\n",
       "      <td>74373</td>\n",
       "      <td>0.777795</td>\n",
       "      <td>1050.221</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>111450</td>\n",
       "      <td>478.310971</td>\n",
       "      <td>298.630592</td>\n",
       "      <td>0.781150</td>\n",
       "      <td>113256</td>\n",
       "      <td>0.690093</td>\n",
       "      <td>1298.188</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>121080</td>\n",
       "      <td>573.403612</td>\n",
       "      <td>270.632507</td>\n",
       "      <td>0.881612</td>\n",
       "      <td>124432</td>\n",
       "      <td>0.723790</td>\n",
       "      <td>1418.385</td>\n",
       "      <td>Besni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>37569</td>\n",
       "      <td>232.427848</td>\n",
       "      <td>208.152006</td>\n",
       "      <td>0.444950</td>\n",
       "      <td>38874</td>\n",
       "      <td>0.794371</td>\n",
       "      <td>734.102</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T14:53:59.776139Z",
     "start_time": "2025-09-28T14:53:59.737466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = df[['Area', 'MajorAxisLength', 'MinorAxisLength', 'Eccentricity', 'ConvexArea', 'Extent', 'Perimeter']]\n",
    "y = df['Class']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ],
   "id": "a1c3751160904881",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 1.2: Manual Scaling and Model Training\n",
    "\n",
    "Here, we manually apply `StandardScaler` and then train an SVM model on the scaled data.\n"
   ],
   "id": "f363f71b077f2540"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T14:59:37.999983Z",
     "start_time": "2025-09-28T14:59:37.981799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Manually scale the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X) # Note: Best practice is to fit only on X_train\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the model on the scaled data\n",
    "model = SVC(kernel='rbf')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "id": "8edc8d921df4bd5f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Besni       0.91      0.83      0.87        83\n",
      "     Kecimen       0.87      0.93      0.90        97\n",
      "\n",
      "    accuracy                           0.88       180\n",
      "   macro avg       0.89      0.88      0.88       180\n",
      "weighted avg       0.88      0.88      0.88       180\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The model achieves an accuracy of **88%**. This process works, but it's cumbersome and requires us to manage the scaled and unscaled data separately.",
   "id": "d1ed94314a4c88db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. The Efficient Workflow: Building a Pipeline\n",
    "\n",
    "Now, let's achieve the same result using a `Pipeline`. We define a series of steps: first, scale the data, and second, apply the SVM classifier.\n"
   ],
   "id": "b6c1599e22aac832"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T14:54:04.377738Z",
     "start_time": "2025-09-28T14:54:04.372245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(kernel='rbf'))\n",
    "])"
   ],
   "id": "5df80c121138dc4d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Using the Pipeline\n",
    "\n",
    "The pipeline object now acts as our model. We can fit it directly on the **original, unscaled training data**. The pipeline will automatically handle the scaling process correctly.\n"
   ],
   "id": "1bb73839e99fad34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T15:01:13.664476Z",
     "start_time": "2025-09-28T15:01:13.648179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fit the entire pipeline on the original (unscaled) training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the original (unscaled) test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "id": "9518e0e72a925209",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Besni       0.91      0.83      0.87        83\n",
      "     Kecimen       0.87      0.93      0.90        97\n",
      "\n",
      "    accuracy                           0.88       180\n",
      "   macro avg       0.89      0.88      0.88       180\n",
      "weighted avg       0.88      0.88      0.88       180\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see, the result is identical to the manual process, but the code is much cleaner and safer.\n",
    "\n",
    "**What happens under the hood?**\n",
    "* When we call `pipeline.fit(X_train, y_train)`, the pipeline first calls `fit_transform` on the `StandardScaler` using `X_train`, then passes the transformed data to the `SVC` model for fitting.\n",
    "* When we call `pipeline.predict(X_test)`, it automatically calls `transform` on the `StandardScaler` using `X_test` and then passes the scaled data to the `SVC`'s `predict` method.\n"
   ],
   "id": "6c500e4357399d22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Why Use a Pipeline?\n",
    "\n",
    "* **Prevents Data Leakage:** The pipeline correctly fits transformers (like `StandardScaler`) on the training data only, preventing information from the test set from influencing the model.\n",
    "* **Simplicity:** It simplifies the code by consolidating multiple steps into a single object. You only need to call `.fit()` and `.predict()` once.\n",
    "* **Reproducibility:** The entire workflow is captured in one object, making it easy to save, load, and reuse, ensuring consistent results.\n",
    "* **Grid Search:** Pipelines are essential for hyperparameter tuning. You can use tools like `GridSearchCV` to simultaneously tune parameters for both the preprocessing steps and the final model."
   ],
   "id": "8f89f3dcb5c30d86"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
