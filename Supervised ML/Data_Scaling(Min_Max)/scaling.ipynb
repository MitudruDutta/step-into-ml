{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The Importance of Feature Scaling in Machine Learning ⚖️\n",
    "\n",
    "**Feature Scaling** is a critical preprocessing step in many machine learning workflows. It involves standardizing the range of the independent variables or features of the data.\n",
    "\n",
    "### Why is Scaling Important?\n",
    "\n",
    "Many machine learning algorithms, especially those that are **distance-based** like Support Vector Machines (SVMs), K-Nearest Neighbors (KNN), and Principal Component Analysis (PCA), are sensitive to the scale of the input features.\n",
    "\n",
    "Consider our Raisin dataset, where `Area` can be in the tens of thousands, while `Eccentricity` is a value between 0 and 1. Without scaling, the `Area` feature would completely dominate any distance-based calculation, and the model might incorrectly assume it's a much more important feature than `Eccentricity`, simply because its numbers are larger.\n",
    "\n",
    "Scaling brings all features to a similar magnitude, ensuring that each one contributes fairly to the model's learning process.\n",
    "\n",
    "In this notebook, we will use **`StandardScaler`**, a common technique that transforms the data so that it has a **mean of 0** and a **standard deviation of 1**. The formula for this is:\n",
    "$$ z = \\frac{x - \\mu}{\\sigma} $$\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation of the feature.\n",
    "\n",
    "---"
   ],
   "id": "7503bcb0ba548230"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Preparing the Raisin Dataset\n",
    "\n",
    "First, we load the Raisin dataset and perform our standard train-test split. The data has features with very different scales.\n"
   ],
   "id": "c5d762ae8591663a"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-28T14:33:43.904627Z",
     "start_time": "2025-09-28T14:33:41.632154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('Raisin_Dataset.xlsx')\n",
    "df.sample(5)"
   ],
   "id": "80239827d78a51ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Area  MajorAxisLength  MinorAxisLength  Eccentricity  ConvexArea  \\\n",
       "696  91464       433.219793       273.255461      0.775982       93852   \n",
       "416  33615       254.472230       171.001050      0.740566       35376   \n",
       "174  51941       349.226170       191.817334      0.835649       53893   \n",
       "56   57127       311.644578       238.641921      0.643138       59943   \n",
       "256  61463       369.399745       213.619620      0.815832       63117   \n",
       "\n",
       "       Extent  Perimeter    Class  \n",
       "696  0.717702   1182.210    Besni  \n",
       "416  0.788049    719.935  Kecimen  \n",
       "174  0.708995    912.259  Kecimen  \n",
       "56   0.693626    952.023  Kecimen  \n",
       "256  0.786777    966.493  Kecimen  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>91464</td>\n",
       "      <td>433.219793</td>\n",
       "      <td>273.255461</td>\n",
       "      <td>0.775982</td>\n",
       "      <td>93852</td>\n",
       "      <td>0.717702</td>\n",
       "      <td>1182.210</td>\n",
       "      <td>Besni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>33615</td>\n",
       "      <td>254.472230</td>\n",
       "      <td>171.001050</td>\n",
       "      <td>0.740566</td>\n",
       "      <td>35376</td>\n",
       "      <td>0.788049</td>\n",
       "      <td>719.935</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>51941</td>\n",
       "      <td>349.226170</td>\n",
       "      <td>191.817334</td>\n",
       "      <td>0.835649</td>\n",
       "      <td>53893</td>\n",
       "      <td>0.708995</td>\n",
       "      <td>912.259</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57127</td>\n",
       "      <td>311.644578</td>\n",
       "      <td>238.641921</td>\n",
       "      <td>0.643138</td>\n",
       "      <td>59943</td>\n",
       "      <td>0.693626</td>\n",
       "      <td>952.023</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>61463</td>\n",
       "      <td>369.399745</td>\n",
       "      <td>213.619620</td>\n",
       "      <td>0.815832</td>\n",
       "      <td>63117</td>\n",
       "      <td>0.786777</td>\n",
       "      <td>966.493</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train-Test Split",
   "id": "af05f4a640be3117"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T14:35:55.865924Z",
     "start_time": "2025-09-28T14:35:55.516751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = df[['Area', 'MajorAxisLength', 'MinorAxisLength', 'Eccentricity', 'ConvexArea', 'Extent', 'Perimeter']]\n",
    "y = df['Class']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ],
   "id": "a1c3751160904881",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Applying StandardScaler\n",
    "\n",
    "Now, we'll use `StandardScaler` from `scikit-learn` to scale our feature data.\n",
    "\n",
    "> **Note on Best Practice:** In this example, the scaler is fit on the entire dataset (`X`). The ideal practice is to **fit the scaler only on the training data (`X_train`)** and then use that same fitted scaler to transform both `X_train` and `X_test`. This prevents any information from the test set from \"leaking\" into the training process.\n",
    "\n"
   ],
   "id": "f363f71b077f2540"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T14:35:58.361547Z",
     "start_time": "2025-09-28T14:35:58.356300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "id": "8edc8d921df4bd5f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here's a look at our scaled test data. Notice how the values are now centered around zero.\n",
   "id": "47b4dccff55200d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T14:36:00.562673Z",
     "start_time": "2025-09-28T14:36:00.558487Z"
    }
   },
   "cell_type": "code",
   "source": "X_test_scaled",
   "id": "f56ca718b3ecf06b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0426657 , -0.14506073,  0.24858118, ..., -0.08106412,\n",
       "         0.35989352, -0.07215554],\n",
       "       [-0.43652993, -0.58876265, -0.10176729, ..., -0.47199211,\n",
       "         0.87296583, -0.59258193],\n",
       "       [-0.85804906, -0.58924598, -1.23173306, ..., -0.8633864 ,\n",
       "         0.00766612, -0.80434516],\n",
       "       ...,\n",
       "       [-0.91228139, -0.85779301, -0.95531349, ..., -0.86908013,\n",
       "         0.27931669, -0.84557505],\n",
       "       [ 0.49085832,  0.58207752,  0.4023557 , ...,  0.46462512,\n",
       "         1.3787713 ,  0.43838459],\n",
       "       [-0.89270747, -0.84114895, -0.95288942, ..., -0.87688445,\n",
       "        -0.14619709, -0.83890868]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Training SVMs on Scaled Data\n",
    "\n",
    "We will now re-train our SVM models using this new scaled data to see how it affects their performance.\n",
    "\n",
    "### a) RBF Kernel with Scaled Data\n",
    "\n",
    "The RBF kernel is highly sensitive to the distance between points, so we expect scaling to have a positive impact.\n"
   ],
   "id": "d1ed94314a4c88db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T14:36:03.625468Z",
     "start_time": "2025-09-28T14:36:03.398497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel='rbf')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "model.n_iter_"
   ],
   "id": "12693e9d09c8006c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Besni       0.91      0.83      0.87        83\n",
      "     Kecimen       0.87      0.93      0.90        97\n",
      "\n",
      "    accuracy                           0.88       180\n",
      "   macro avg       0.89      0.88      0.88       180\n",
      "weighted avg       0.88      0.88      0.88       180\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([419], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Result:** The accuracy improved from **83% (unscaled) to 88% (scaled)**. Scaling the features allowed the RBF kernel to better distinguish between the classes.\n",
   "id": "805f3fee08a2a025"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### b) Linear Kernel with Scaled Data\n",
    "\n",
    "Let's see how scaling affects the linear kernel, which was our best performer on the unscaled data.\n"
   ],
   "id": "9d23ac39c627034d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T14:36:18.791717Z",
     "start_time": "2025-09-28T14:36:18.776512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "model.n_iter_"
   ],
   "id": "f4ee08403587df7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Besni       0.90      0.84      0.87        83\n",
      "     Kecimen       0.87      0.92      0.89        97\n",
      "\n",
      "    accuracy                           0.88       180\n",
      "   macro avg       0.88      0.88      0.88       180\n",
      "weighted avg       0.88      0.88      0.88       180\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2164], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Result:** The accuracy slightly decreased from **91% (unscaled) to 88% (scaled)**. This is an interesting outcome. While scaling is generally a best practice, it doesn't *guarantee* a better score for every model. In this case, the unscaled data was already very well-separated by a linear hyperplane, and scaling slightly changed the geometry, making the separation marginally less effective. However, the performance is still very strong.\n",
   "id": "88b5872df7a7462e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "* Feature scaling is a critical preprocessing step, especially for distance-based algorithms like SVM with an RBF kernel.\n",
    "* For the **RBF kernel**, scaling significantly improved the model's accuracy.\n",
    "* For the **linear kernel**, the impact was minimal on this specific dataset, but scaling is still recommended to ensure all features are treated equally.\n",
    "* Always **fit your scaler on the training data only** to prevent data leakage and ensure a robust evaluation of your model."
   ],
   "id": "b9032208ade98dbf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
