# ğŸš€ Step Into Machine Learning

> **A comprehensive, hands-on journey through Machine Learning fundamentals**

Welcome to your complete Machine Learning learning workspace! This repository contains carefully crafted Jupyter notebooks, real datasets, and step-by-step tutorials designed to take you from ML beginner to practitioner. Whether you're a student, data enthusiast, or professional looking to upskill, this repository provides a structured path through the essential concepts of Machine Learning.

## ğŸ¯ What You'll Learn

This repository covers the complete Machine Learning pipeline:

- **ğŸ“Š Supervised Learning**: Regression and Classification algorithms
- **ğŸ” Unsupervised Learning**: Clustering and pattern discovery
- **âš™ï¸ Feature Engineering**: Data preprocessing and feature selection
- **ğŸ›ï¸ Model Evaluation**: Cross-validation, metrics, and hyperparameter tuning
- **ğŸŒŸ Ensemble Methods**: Combining models for better performance
- **ğŸ› ï¸ MLOps Basics**: Pipelines, scaling, and best practices

## ğŸ—ï¸ Repository Structure

```
ğŸ“ Step-Into-Machine-Learning/
â”œâ”€â”€ ğŸ“‚ Supervised ML/              # Core supervised learning algorithms
â”œâ”€â”€ ğŸ“‚ Unsupervised Learning/      # Clustering and dimensionality reduction
â”œâ”€â”€ ğŸ“‚ Feature Engineering/        # Data preprocessing techniques
â”œâ”€â”€ ğŸ“‚ Model Evaluation and Fine Tuning/  # Validation and optimization
â”œâ”€â”€ ğŸ“‚ Ensemble Learning/          # Advanced ensemble methods
â”œâ”€â”€ ğŸ“„ requirements.txt            # Python dependencies
â””â”€â”€ ğŸ“„ README.md                   # This comprehensive guide
```

## ğŸŒŸ Key Features

- **ğŸ“š Theory + Practice**: Each topic includes mathematical foundations and hands-on implementation
- **ğŸ”¬ Real Datasets**: Work with actual data from various domains
- **ğŸ“ Detailed Explanations**: Comprehensive markdown documentation in every notebook
- **ğŸ¯ Progressive Learning**: Topics build upon each other logically
- **ğŸ’» Production-Ready Code**: Industry best practices and clean implementations
- **ğŸ”„ Reproducible Results**: Fixed random seeds and clear instructions

---

## ğŸ“‹ Complete Learning Path

### ğŸ“ Supervised Learning Fundamentals

#### ğŸ“ˆ Regression Algorithms
| Topic | Notebook | Description |
|-------|----------|-------------|
| **Simple Linear Regression** | [ğŸ““ linear_regression_single_variable.ipynb](Supervised%20ML/Simple_linear_regression/linear_regression_single_variable.ipynb) | Foundation of ML - predicting with one feature |
| **Multiple Linear Regression** | [ğŸ““ linear_regression_mul_var.ipynb](Supervised%20ML/Multiple_linear_regression/linear_regression_mul_var.ipynb) | Extending to multiple features and interactions |
| **Polynomial Regression** | [ğŸ““ poly_regression.ipynb](Supervised%20ML/Polynomial_Regression/poly_regression.ipynb) | Capturing non-linear relationships |
| **Regularization (Ridge/Lasso)** | [ğŸ““ l1l2regularization.ipynb](Supervised%20ML/L1L2Regularization/l1l2regularization.ipynb) | Preventing overfitting with L1/L2 penalties |

#### ğŸ¯ Classification Algorithms
| Topic | Notebook | Description |
|-------|----------|-------------|
| **Logistic Regression** | [ğŸ““ logisticregression.ipynb](Supervised%20ML/LogisticRegression/logisticregression.ipynb) | Binary classification fundamentals |
| **Multiclass Classification** | [ğŸ““ multiclass_class.ipynb](Supervised%20ML/Multiclass_classification/multiclass_class.ipynb) | Iris dataset - multi-class problems |
| **Decision Trees** | [ğŸ““ decisiontree.ipynb](Supervised%20ML/Decision_tree/decisiontree.ipynb) | Interpretable tree-based decisions |
| **Support Vector Machine** | [ğŸ““ SVM.ipynb](Supervised%20ML/SVM/SVM.ipynb) | Maximum margin classification |
| **Naive Bayes** | [ğŸ““ smsclassifier.ipynb](Supervised%20ML/NaiveBayes_SMSSpamClassification_/smsclassifier.ipynb) | Probabilistic classification for text |

#### ğŸ“Š Model Evaluation & Validation
| Topic | Notebook | Description |
|-------|----------|-------------|
| **Train/Test Split** | [ğŸ““ train_test_split.ipynb](Supervised%20ML/Train_Test_Split/train_test_split.ipynb) | Proper data splitting strategies |
| **Precision & Recall** | [ğŸ““ precision_recall.ipynb](Supervised%20ML/Precision_Recall/precision_recall.ipynb) | Understanding classification metrics |
| **F1 Score & Confusion Matrix** | [ğŸ““ f1_confusion_matrix.ipynb](Supervised%20ML/F1_ConfusionMatrix/f1_confusion_matrix.ipynb) | Comprehensive evaluation metrics |
| **ROC Curve & AUC** | [ğŸ““ rocauc.ipynb](Model%20Evaluation%20and%20Fine%20Tuning/ROCAUC/rocauc.ipynb) | ROC analysis and model comparison |
| **K-Fold Cross-Validation** | [ğŸ““ kfold.ipynb](Model%20Evaluation%20and%20Fine%20Tuning/KFoldCross/kfold.ipynb) | Robust model validation |
| **Stratified K-Fold** | [ğŸ““ stratifiedkfoldcross.ipynb](Model%20Evaluation%20and%20Fine%20Tuning/StratifiedKFold/stratifiedkfoldcross.ipynb) | Balanced cross-validation |
| **Hyperparameter Tuning** | [ğŸ““ gridsearchCV.ipynb](Model%20Evaluation%20and%20Fine%20Tuning/HyperParameter%20Tuning/gridsearchCV.ipynb) â€¢ [ğŸ““ randomisedsearchCV.ipynb](Model%20Evaluation%20and%20Fine%20Tuning/HyperParameter%20Tuning/randomisedsearchCV.ipynb) | Grid & Random search optimization |

#### ğŸ› ï¸ Data Preprocessing & Pipelines
| Topic | Notebook | Description |
|-------|----------|-------------|
| **Feature Scaling** | [ğŸ““ scaling.ipynb](Supervised%20ML/Data_Scaling(Min_Max)/scaling.ipynb) | Min-Max & Standard normalization |
| **One-Hot Encoding** | [ğŸ““ one_hot_encoding.ipynb](Supervised%20ML/One_Hot_Encoding/one_hot_encoding.ipynb) | Handling categorical variables |
| **Scikit-Learn Pipeline** | [ğŸ““ sklearnpipeline.ipynb](Supervised%20ML/SklearnPipeline/sklearnpipeline.ipynb) | Production-ready ML workflows |
| **Customer Churn Analysis** | [ğŸ““ churnprediction.ipynb](Supervised%20ML/CustomerChurn/churnprediction.ipynb) | Real-world imbalanced classification |

### ğŸŒŸ Advanced Ensemble Learning
| Topic | Notebook | Description |
|-------|----------|-------------|
| **Random Forest** | [ğŸ““ randomforestclassification.ipynb](Ensemble%20Learning/RandomForest/randomforestclassification.ipynb) | Bootstrap aggregating with decision trees |
| **Gradient Boosting** | [ğŸ““ gradientboostclass.ipynb](Ensemble%20Learning/Gradient%20Boosting/gradientboostclass.ipynb) â€¢ [ğŸ““ gradientboostreg.ipynb](Ensemble%20Learning/Gradient%20Boosting/gradientboostreg.ipynb) | Sequential learning for classification & regression |
| **XGBoost** | [ğŸ““ xgboostclassifier.ipynb](Ensemble%20Learning/XGBoost/xgboostclassifier.ipynb) â€¢ [ğŸ““ xgboostregressor.ipynb](Ensemble%20Learning/XGBoost/xgboostregressor.ipynb) | Extreme gradient boosting optimization |
| **Voting Ensembles** | [ğŸ““ ensemble_voting_classifier.ipynb](Ensemble%20Learning/Majority%2C%20Average%2C%20Weighted%20Average/ensemble_voting_classifier.ipynb) â€¢ [ğŸ““ ensemble_voting_regressor.ipynb](Ensemble%20Learning/Majority%2C%20Average%2C%20Weighted%20Average/ensemble_voting_regressor.ipynb) | Combining multiple models effectively |

### âš™ï¸ Feature Engineering & Selection
| Topic | Notebook | Description |
|-------|----------|-------------|
| **Correlation Analysis** | [ğŸ““ featureusingcorr.ipynb](Feature%20Engineering/Correlation/featureusingcorr.ipynb) | Feature selection using correlation matrices |
| **Variance Inflation Factor** | [ğŸ““ vif.ipynb](Feature%20Engineering/Variance%20Inflation%20Factor/vif.ipynb) | Detecting and handling multicollinearity |
| **K-Means for Features** | [ğŸ““ kmeans.ipynb](Unsupervised%20Learning/KMeans/kmeans.ipynb) | Creating cluster-based features |

### ğŸ” Unsupervised Learning & Clustering
| Topic | Notebook | Description |
|-------|----------|-------------|
| **K-Means Clustering** | [ğŸ““ kmeans.ipynb](Unsupervised%20Learning/KMeans/kmeans.ipynb) | Centroid-based customer segmentation |
| **Hierarchical Clustering** | [ğŸ““ hc.ipynb](Unsupervised%20Learning/Hierarchical%20Clustering/hc.ipynb) | Tree-based clustering with dendrograms |
| **DBSCAN Clustering** | [ğŸ““ dbscan.ipynb](Unsupervised%20Learning/DBSCAN/dbscan.ipynb) â€¢ [ğŸ““ synthetic_data.ipynb](Unsupervised%20Learning/DBSCAN/synthetic_data.ipynb) | Density-based clustering with outlier detection |

> ğŸ’¡ **Pro Tip**: Each folder contains detailed README files with theory, implementation notes, and best practices!

---

## ğŸ§  What is Machine Learning?

Machine Learning is the science of enabling computers to learn and make decisions from data without being explicitly programmed for every scenario. This repository focuses on:

- **ğŸ¯ Supervised Learning**: Learning from labeled examples to predict outcomes
- **ğŸ” Unsupervised Learning**: Discovering hidden patterns in unlabeled data  
- **ğŸ“Š Statistical Foundations**: Understanding the math behind the algorithms
- **ğŸ’¼ Real-World Applications**: Practical implementations with business context

### ğŸ› ï¸ Tech Stack
- **Python 3.9+**: Core programming language
- **Pandas & NumPy**: Data manipulation and numerical computing
- **Scikit-learn**: Machine learning algorithms and tools
- **Matplotlib & Seaborn**: Data visualization and plotting
- **Jupyter Notebooks**: Interactive development environment

---

## Linear Regression essentials 

- Simple Linear Regression (one feature):
  - Model: y = Î²â‚€ + Î²â‚ x + Îµ
  - Closed-form fit:
    - Î²Ì‚â‚ = Cov(x, y) / Var(x)
    - Î²Ì‚â‚€ = È³ âˆ’ Î²Ì‚â‚ xÌ„
  - Common metrics:
    - MSE = (1/n) Î£ (yáµ¢ âˆ’ Å·áµ¢)Â²
    - RMSE = âˆšMSE
    - RÂ² = 1 âˆ’ Î£(y âˆ’ Å·)Â² / Î£(y âˆ’ È³)Â²

- Multiple Linear Regression (many features):
  - Model: y = Î²â‚€ + Î²â‚xâ‚ + â€¦ + Î²_px_p + Îµ (vector form: y = XÎ² + Îµ)
  - Ordinary Least Squares (if Xáµ€X invertible): Î²Ì‚ = (Xáµ€X)â»Â¹ Xáµ€ y
  - Practical notes: check multicollinearity, scale features when needed, evaluate with train/test split.

- Polynomial Regression (nonlinear in x, linear in parameters):
  - Model (degree k): y = Î²â‚€ + Î²â‚ x + Î²â‚‚ xÂ² + â€¦ + Î²_k x^k + Îµ
  - View as linear regression on polynomial features Î¦(x) = [1, x, xÂ², â€¦, x^k]
  - OLS on transformed design matrix: Î²Ì‚ = (Î¦áµ€Î¦)â»Â¹ Î¦áµ€ y
  - Practice: generate features via `sklearn.preprocessing.PolynomialFeatures`, fit with `LinearRegression`; tune degree with crossâ€‘validation to avoid overfitting.

Use scikit-learn for practical training/evaluation:
- SLR/MLR/Poly: `from sklearn.linear_model import LinearRegression`, `from sklearn.preprocessing import PolynomialFeatures`
- Split & metrics: `from sklearn.model_selection import train_test_split`, `from sklearn.metrics import mean_squared_error, r2_score`

---

## Regularization essentials

Regularization adds a penalty to control model complexity and reduce overfitting.

- Ridge (L2):
  - Objective: min_Î² (1/2n) ||y âˆ’ XÎ²||Â² + Î± ||Î²||Â²â‚‚
  - Closed-form: Î²Ì‚ = (Xáµ€X + 2nÎ± I)â»Â¹ Xáµ€ y
  - Properties: shrinks coefficients smoothly; keeps all features; sensitive to feature scale.

- Lasso (L1):
  - Objective: min_Î² (1/2n) ||y âˆ’ XÎ²||Â² + Î± ||Î²||â‚
  - Solution: no closed-form; solved by coordinate descent/ISTA; can set some Î²_j exactly to 0 (feature selection).

Tips:
- Standardize features first: `StandardScaler()` inside a `Pipeline` with `Ridge`/`Lasso`.
- Tune Î± via cross-validation: `RidgeCV`, `LassoCV`.

---

## Multicollinearity & VIF essentials

Multicollinearity occurs when predictor variables are highly correlated, causing unstable coefficient estimates in linear models.

- Variance Inflation Factor (VIF):
  - Formula: VIF_i = 1 / (1 - RÂ²_i), where RÂ²_i is from regressing feature i against all other features
  - Interpretation:
    - VIF = 1: No correlation
    - 1 < VIF < 5: Moderate correlation (acceptable)
    - 5 â‰¤ VIF < 10: High correlation (concerning)
    - VIF â‰¥ 10: Very high correlation (action required)

- Detection & Solutions:
  - Calculate VIF: `from statsmodels.stats.outliers_influence import variance_inflation_factor`
  - Remove features with VIF > 10 iteratively (highest first)
  - Alternative: Use regularization (Ridge/Lasso) which handles multicollinearity naturally
  - Always recalculate VIF after removing features

Practice: Check VIF before finalizing linear regression models; prioritize model interpretability and stability over minor performance gains.

---

## K-Means Clustering essentials

K-Means is a centroid-based unsupervised learning algorithm that partitions data into k clusters.

- Algorithm Overview:
  - Objective: minimize within-cluster sum of squares (WCSS)
  - Formula: J = Î£(i=1 to k) Î£(xâˆˆCi) ||x - Î¼i||Â²
  - Steps: Initialize centroids â†’ Assign points â†’ Update centroids â†’ Repeat until convergence

- Key Considerations:
  - **Feature Scaling**: Always scale features (StandardScaler/MinMaxScaler) since K-Means uses Euclidean distance
  - **Choosing k**: Use elbow method, silhouette analysis, or domain knowledge
  - **Initialization**: K-Means++ (default in scikit-learn) provides better starting centroids

- Applications:
  - **Customer Segmentation**: Group customers by behavior/demographics
  - **Feature Engineering**: Create cluster-based features, dimensionality reduction
  - **Market Research**: Identify distinct market segments
  - **Anomaly Detection**: Points far from centroids may be outliers

- Implementation:
  - Basic: `from sklearn.cluster import KMeans`
  - Evaluation: `from sklearn.metrics import silhouette_score`
  - Preprocessing: Always use `StandardScaler()` or `MinMaxScaler()`

Practice: Scale features first, determine optimal k, validate business relevance of clusters, and use centroids for segment profiling.

---

## Hierarchical Clustering essentials

Hierarchical clustering builds a tree of clusters without requiring a pre-specified number of clusters.

- Algorithm Types:
  - **Agglomerative (Bottom-up)**: Start with individual points, merge closest clusters iteratively
  - **Divisive (Top-down)**: Start with all points, split recursively (less common)

- Linkage Criteria (distance between clusters):
  - **Single**: Minimum distance between any two points
  - **Complete**: Maximum distance between any two points  
  - **Average**: Average distance between all point pairs
  - **Ward**: Minimizes within-cluster variance (most common)

- Key Concepts:
  - **Dendrogram**: Tree diagram showing cluster merging hierarchy
  - **Cutting Height**: Determines final number of clusters
  - **Cophenetic Distance**: Distance at which clusters merge

- Advantages:
  - No need to specify number of clusters beforehand
  - Produces interpretable dendrogram
  - Deterministic results (unlike K-Means)
  - Works well with small datasets

- Implementation:
  - Scikit-learn: `from sklearn.cluster import AgglomerativeClustering`
  - SciPy: `from scipy.cluster.hierarchy import dendrogram, linkage`
  - Visualization: `dendrogram()` for tree structure analysis

Practice: Use Ward linkage for most cases, analyze dendrogram to choose optimal cluster count, consider computational complexity O(nÂ³) for large datasets.

---

## DBSCAN Clustering essentials

DBSCAN (Density-Based Spatial Clustering of Applications with Noise) finds clusters based on density and automatically identifies outliers.

- Key Parameters:
  - **eps (Îµ)**: Maximum distance between two samples to be neighbors
  - **min_samples**: Minimum points required to form a dense region (core point)

- Core Concepts:
  - **Core Point**: Has at least min_samples neighbors within eps distance
  - **Border Point**: Within eps of a core point but not core itself
  - **Noise Point**: Neither core nor border (labeled as -1)

- Algorithm Steps:
  1. Identify core points (â‰¥ min_samples neighbors within eps)
  2. Form clusters by connecting core points and their neighbors
  3. Assign border points to nearest cluster
  4. Mark remaining points as noise/outliers

- Key Advantages:
  - **Automatic cluster detection**: No need to specify number of clusters
  - **Arbitrary shapes**: Can find non-spherical clusters
  - **Outlier detection**: Automatically identifies noise points
  - **Robust to outliers**: Outliers don't affect cluster formation

- Parameter Selection:
  - **eps**: Use k-distance plot or domain knowledge
  - **min_samples**: Rule of thumb: 2Ã—dimensions, minimum 3-4
  - **Feature scaling**: Important since DBSCAN uses distance metrics

- Applications:
  - Anomaly detection in fraud/security
  - Image processing and computer vision
  - Geolocation clustering
  - Market segmentation with outlier identification

- Implementation:
  - Basic: `from sklearn.cluster import DBSCAN`
  - Preprocessing: Always scale features with `StandardScaler()`
  - Evaluation: Silhouette score (excluding noise points)

Practice: Experiment with eps/min_samples on scaled data, analyze noise points for insights, validate clusters make business sense.

---

## Classification metrics essentials

Confusion Matrix layout:

|            | Predicted 0 | Predicted 1 |
|------------|-------------|-------------|
| Actual 0   | TN          | FP          |
| Actual 1   | FN          | TP          |

Key formulas (binary):
- Precision = TP / (TP + FP)
- Recall (Sensitivity) = TP / (TP + FN)
- F1-Score = 2 * (Precision * Recall) / (Precision + Recall) = 2TP / (2TP + FP + FN)
- Accuracy = (TP + TN) / (TP + TN + FP + FN)

When to favor which:
- High Precision: costly false positives (e.g. spam mislabeling important email)
- High Recall: costly false negatives (e.g. disease screening)
- F1: need balance under class imbalance

(Deeper narrative lives in each topic folder; this section stays concise.)

---

## ğŸ“… 21-Day Learning Journey

Follow this structured path to master Machine Learning fundamentals:

### Week 1: Foundations ğŸ—ï¸
| Day | Topic | Focus |
|-----|-------|-------|
| 1 | Simple Linear Regression | Mathematical foundations |
| 2 | Train/Test Split & Metrics | Proper evaluation techniques |
| 3 | Multiple Linear Regression | Multi-dimensional relationships |
| 4 | Polynomial Regression | Non-linear pattern capture |
| 5 | Regularization (Ridge/Lasso) | Overfitting prevention |
| 6 | Logistic Regression | Classification fundamentals |
| 7 | Decision Trees | Interpretable ML models |

### Week 2: Advanced Techniques ğŸš€
| Day | Topic | Focus |
|-----|-------|-------|
| 8 | Precision/Recall & F1 | Classification metrics mastery |
| 9 | ROC Curves & AUC | Model comparison techniques |
| 10 | Support Vector Machines | Maximum margin classification |
| 11 | Naive Bayes | Probabilistic classification |
| 12 | Feature Scaling & Encoding | Data preprocessing |
| 13 | Scikit-Learn Pipelines | Production workflows |
| 14 | Customer Churn Analysis | Real-world case study |

### Week 3: Ensemble & Unsupervised ğŸŒŸ
| Day | Topic | Focus |
|-----|-------|-------|
| 15 | Random Forest | Bootstrap aggregating |
| 16 | Gradient Boosting | Sequential learning |
| 17 | XGBoost | Advanced boosting |
| 18 | Voting Ensembles | Model combination |
| 19 | K-Means Clustering | Customer segmentation |
| 20 | Hierarchical Clustering | Tree-based clustering |
| 21 | DBSCAN Clustering | Density-based methods |

> ğŸ“š **Flexible Learning**: Adapt the pace to your schedule. Each topic builds logically on previous concepts.

---

## ğŸš€ Quick Start Guide

### Prerequisites
- **Python 3.9+** (recommended: Python 3.10 or 3.11)
- **Git** (optional, for cloning)
- **8GB RAM** minimum (16GB recommended for larger datasets)

### Installation

#### Option 1: Clone Repository
```bash
git clone https://github.com/your-username/Step-Into-Machine-Learning.git
cd Step-Into-Machine-Learning
```

#### Option 2: Download ZIP
Download and extract the repository ZIP file from GitHub.

### Environment Setup

#### Windows (Command Prompt)
```batch
python -m venv ml-env
ml-env\Scripts\activate
pip install --upgrade pip
pip install -r requirements.txt
python -m ipykernel install --user --name ml-workspace --display-name "Python (ML Workspace)"
```

#### macOS/Linux (Terminal)
```bash
python3 -m venv ml-env
source ml-env/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
python -m ipykernel install --user --name ml-workspace --display-name "Python (ML Workspace)"
```

### Launch Jupyter
```bash
jupyter notebook
```

### ğŸ”§ Troubleshooting
- **XGBoost Issues**: Ensure you have the latest pip and Python version
- **Excel Files**: `openpyxl` is included in requirements for `.xlsx` support
- **Memory Issues**: Close other applications when working with larger datasets
- **Kernel Issues**: Restart Jupyter and select "Python (ML Workspace)" kernel

---

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

- **ğŸ› Bug Reports**: Found an issue? Open a GitHub issue
- **ğŸ“š Documentation**: Improve explanations or add examples
- **ğŸ’¡ New Topics**: Suggest additional ML concepts to cover
- **ğŸ”§ Code Improvements**: Optimize implementations or add features

### Contribution Guidelines
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“Š Repository Stats

- **ğŸ“ 50+ Jupyter Notebooks**: Comprehensive coverage of ML topics
- **ğŸ“ˆ 20+ Algorithms**: From linear regression to advanced ensembles  
- **ğŸ¯ Real Datasets**: Industry-relevant examples and case studies
- **ğŸ“š Detailed Documentation**: Theory + implementation for each topic
- **ğŸ”„ Regular Updates**: Continuously improved content and examples

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Scikit-learn Team**: For the excellent ML library
- **Jupyter Project**: For the interactive notebook environment
- **Python Community**: For the amazing ecosystem of data science tools
- **Contributors**: Everyone who helps improve this learning resource

---

## ğŸ“ Support & Community

- **ğŸ“§ Questions**: Open a GitHub issue for technical questions
- **ğŸ’¬ Discussions**: Use GitHub Discussions for general ML topics
- **ğŸ¦ Updates**: Follow for the latest additions and improvements
- **â­ Star**: If this repository helps you, please give it a star!

---

**Happy Learning! ğŸ‰**

*Master Machine Learning one algorithm at a time.*

---
_Last updated: 2025-10-23 | Version: 2.0_
